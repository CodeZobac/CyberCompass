# Quick Start Guide - Unit Tests

## ğŸš€ Run Tests Immediately

```bash
# Navigate to ai-backend directory
cd ai-backend

# Ensure dependencies are synced
uv sync --extra test --extra dev

# Run all unit tests
uv run pytest tests/unit/ -v
```

## ğŸ“Š Validate Test Structure

```bash
cd ai-backend/tests/unit
uv run python validate_tests.py
```

Expected output:
```
âœ… Has 50 test methods (target: 50+)
âœ… Has 20 test classes (target: 15+)
âœ… Has 3 test files (target: 3)
ğŸ‰ All validation checks passed!
```

## ğŸ¯ Run Specific Tests

### By Test File
```bash
# Agent tests
uv run python tests/unit/test_runner.py --agents

# API endpoint tests
uv run python tests/unit/test_runner.py --api

# WebSocket tests
uv run python tests/unit/test_runner.py --websocket
```

### By Test Class
```bash
# Test a specific agent
uv run pytest tests/unit/test_agents.py::TestEthicsMentorAgent -v

# Test authentication endpoints
uv run pytest tests/unit/test_api_endpoints.py::TestAuthenticationEndpoints -v

# Test WebSocket connections
uv run pytest tests/unit/test_websocket.py::TestWebSocketConnection -v
```

### By Test Method
```bash
# Test agent initialization
uv run pytest tests/unit/test_agents.py::TestEthicsMentorAgent::test_agent_initialization -v

# Test root endpoint
uv run pytest tests/unit/test_api_endpoints.py::TestRootEndpoints::test_root_endpoint -v

# Test WebSocket connection
uv run pytest tests/unit/test_websocket.py::TestWebSocketConnection::test_websocket_connect_success -v
```

## ğŸ“ˆ Generate Coverage Report

```bash
# Run tests with coverage
uv run python tests/unit/test_runner.py --coverage

# View HTML report (opens in browser)
# Linux/macOS:
open htmlcov/index.html

# Windows:
start htmlcov/index.html
```

## ğŸ” Common Commands

```bash
# Run all tests with short output
uv run pytest tests/unit/ -v --tb=short

# Run tests and stop on first failure
uv run pytest tests/unit/ -v -x

# Run tests matching a pattern
uv run pytest tests/unit/ -v -k "agent"

# Run tests with detailed output
uv run pytest tests/unit/ -vv

# Run tests quietly (only show failures)
uv run pytest tests/unit/ -q
```

## ğŸ“ Test Files Overview

| File | Tests | Focus |
|------|-------|-------|
| `test_agents.py` | 14 | CrewAI agents, multilingual support |
| `test_api_endpoints.py` | 29 | FastAPI endpoints, validation, security |
| `test_websocket.py` | 7 | WebSocket connections, message routing |

## âœ… Requirements Covered

- âœ… Tests for individual CrewAI agents and their responses
- âœ… Tests for FastAPI endpoints with various input scenarios
- âœ… Tests for WebSocket connection handling and message routing

## ğŸ› ï¸ Troubleshooting

### Missing Dependencies
```bash
uv sync --extra test --extra dev
```

### Import Errors
```bash
# Ensure you're in the ai-backend directory
cd ai-backend
uv run pytest tests/unit/ -v
```

### Test Discovery Issues
```bash
# Run from project root
cd ai-backend
uv run pytest tests/unit/ -v
```

## ğŸ“š More Information

- Full documentation: `README.md`
- Test summary: `TEST_SUMMARY.md`
- Implementation details: `IMPLEMENTATION_SUMMARY.md`

## ğŸ‰ Success Criteria

Your tests are working correctly if:
1. âœ… Validation script shows 50+ test methods
2. âœ… All tests can be discovered by pytest
3. âœ… No import errors when running tests
4. âœ… Test runner executes without errors

---

**Note**: These tests use mocking, so they don't require actual AI models or external services to run.
